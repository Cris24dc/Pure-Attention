Deci fac un proiect in care fac un MultiLayer Perceptron in CUDA si C++ cu binding in python. Dar problema e ca ori am niste erori de implementare, ori de concurenta, ori de kernels, ori niste erori la graful de autograd, ori niste erori care tin de teorie ML, deoarece gradientii pare ca nu se comporta corect, daca cresc learning rate loss-ul se comporta instabil, iar predictiile finale nu sunt satisfacatoare. O sa iti las tot codul ca sa intelegi contextul si vreau sfaturi de imbunatatiri sau corectii. 

Output curent:
Se încarcă datele California Housing...
Start training on 5000 samples with LR=1e-05...

--- Initial Weights Stats ---
[L1 Weights] DATA | Mean: 0.00298 | Std: 0.49641 | Min: -1.61923 | Max: 1.54362
[L1 Bias] DATA | Mean: 0.00000 | Std: 0.00000 | Min: 0.00000 | Max: 0.00000
[L2 Weights] DATA | Mean: -0.00006 | Std: 0.08850 | Min: -0.36148 | Max: 0.35235
--------------------------------------------------

Epoch 0 | Loss: 2.756659
[Predictions] DATA | Mean: -0.64555 | Std: 1.04124 | Min: -27.58459 | Max: 1.25258
[L2 W Grad] GRAD | Mean: 0.00371 | Std: 0.06532 | Min: -0.71191 | Max: 0.71930
[L2 B Grad] GRAD | Mean: 0.00469 | Std: 0.06671 | Min: -0.24378 | Max: 0.23046
[L1 W Grad] GRAD | Mean: 0.00075 | Std: 0.05478 | Min: -0.25769 | Max: 0.38841
[L1 B Grad] GRAD | Mean: 0.00613 | Std: 0.06224 | Min: -0.18638 | Max: 0.27423

Epoch 100 | Loss: 1.011061
[Predictions] DATA | Mean: 0.12327 | Std: 0.52035 | Min: -7.56797 | Max: 1.95915
[L2 W Grad] GRAD | Mean: -0.00104 | Std: 2.03532 | Min: -25.42494 | Max: 20.10886
[L2 B Grad] GRAD | Mean: -0.02120 | Std: 1.67302 | Min: -7.39765 | Max: 7.18322
[L1 W Grad] GRAD | Mean: -0.05218 | Std: 1.88507 | Min: -12.19677 | Max: 15.39347
[L1 B Grad] GRAD | Mean: -0.02688 | Std: 1.84896 | Min: -5.03003 | Max: 8.71523

Epoch 200 | Loss: 0.824645
[Predictions] DATA | Mean: 0.31646 | Std: 0.89711 | Min: -2.30322 | Max: 8.22868
[L2 W Grad] GRAD | Mean: -0.10844 | Std: 2.03565 | Min: -30.23004 | Max: 21.64902
[L2 B Grad] GRAD | Mean: -0.20228 | Std: 1.53856 | Min: -6.22650 | Max: 7.74025
[L1 W Grad] GRAD | Mean: -0.11760 | Std: 2.14582 | Min: -17.74406 | Max: 19.12760
[L1 B Grad] GRAD | Mean: -0.20770 | Std: 1.97854 | Min: -5.33613 | Max: 8.50140

Epoch 300 | Loss: 1.117669
[Predictions] DATA | Mean: 0.01490 | Std: 1.40316 | Min: -4.63112 | Max: 11.42251
[L2 W Grad] GRAD | Mean: -0.11713 | Std: 1.41832 | Min: -21.26121 | Max: 20.05653
[L2 B Grad] GRAD | Mean: -0.23666 | Std: 1.46410 | Min: -7.21371 | Max: 5.40850
[L1 W Grad] GRAD | Mean: -0.07018 | Std: 1.52227 | Min: -13.61144 | Max: 13.52510
[L1 B Grad] GRAD | Mean: -0.23258 | Std: 1.63975 | Min: -5.61133 | Max: 5.22755

Epoch 400 | Loss: 1.209730
[Predictions] DATA | Mean: -0.17893 | Std: 1.55941 | Min: -5.95916 | Max: 8.86561
[L2 W Grad] GRAD | Mean: -0.01435 | Std: 1.13219 | Min: -14.06406 | Max: 17.21765
[L2 B Grad] GRAD | Mean: -0.06730 | Std: 0.80426 | Min: -3.70185 | Max: 3.36493
[L1 W Grad] GRAD | Mean: -0.01063 | Std: 1.25509 | Min: -7.56318 | Max: 6.20084
[L1 B Grad] GRAD | Mean: -0.06197 | Std: 1.20285 | Min: -6.68773 | Max: 3.15414

Epoch 500 | Loss: 0.916958
[Predictions] DATA | Mean: -0.02024 | Std: 1.40903 | Min: -5.68115 | Max: 8.37199
[L2 W Grad] GRAD | Mean: 0.08006 | Std: 1.44755 | Min: -20.65246 | Max: 19.04457
[L2 B Grad] GRAD | Mean: 0.09357 | Std: 1.23083 | Min: -4.83603 | Max: 5.00555
[L1 W Grad] GRAD | Mean: 0.03798 | Std: 1.60676 | Min: -9.67238 | Max: 8.63605
[L1 B Grad] GRAD | Mean: 0.09568 | Std: 1.43306 | Min: -6.17947 | Max: 4.95301

Epoch 600 | Loss: 0.697088
[Predictions] DATA | Mean: 0.16599 | Std: 1.04581 | Min: -10.37866 | Max: 7.89232
[L2 W Grad] GRAD | Mean: 0.08725 | Std: 1.68260 | Min: -20.30622 | Max: 24.42633
[L2 B Grad] GRAD | Mean: 0.09119 | Std: 1.36902 | Min: -5.22707 | Max: 6.08918
[L1 W Grad] GRAD | Mean: 0.06462 | Std: 1.75336 | Min: -12.37796 | Max: 13.54427
[L1 B Grad] GRAD | Mean: 0.04652 | Std: 1.60529 | Min: -6.35460 | Max: 5.52707

Epoch 700 | Loss: 0.757604
[Predictions] DATA | Mean: 0.13818 | Std: 0.66255 | Min: -7.38397 | Max: 10.62545
[L2 W Grad] GRAD | Mean: 0.02231 | Std: 1.52769 | Min: -14.74088 | Max: 21.36610
[L2 B Grad] GRAD | Mean: -0.02872 | Std: 1.08301 | Min: -2.91182 | Max: 4.25996
[L1 W Grad] GRAD | Mean: 0.04617 | Std: 1.60042 | Min: -10.81480 | Max: 10.59168
[L1 B Grad] GRAD | Mean: -0.13538 | Std: 1.58784 | Min: -7.96265 | Max: 5.50260

Epoch 800 | Loss: 1.047706
[Predictions] DATA | Mean: -0.03549 | Std: 0.56013 | Min: -3.54256 | Max: 6.98195
[L2 W Grad] GRAD | Mean: -0.04658 | Std: 1.05400 | Min: -10.57234 | Max: 15.75072
[L2 B Grad] GRAD | Mean: -0.12527 | Std: 0.83236 | Min: -3.51242 | Max: 4.19066
[L1 W Grad] GRAD | Mean: 0.01252 | Std: 1.25354 | Min: -6.82602 | Max: 5.19375
[L1 B Grad] GRAD | Mean: -0.27196 | Std: 1.37402 | Min: -10.03344 | Max: 3.96727

Epoch 900 | Loss: 1.013839
[Predictions] DATA | Mean: -0.12682 | Std: 0.62580 | Min: -2.98400 | Max: 9.97657
[L2 W Grad] GRAD | Mean: -0.08254 | Std: 1.21741 | Min: -14.46027 | Max: 13.83717
[L2 B Grad] GRAD | Mean: -0.17609 | Std: 1.04112 | Min: -4.68281 | Max: 4.68109
[L1 W Grad] GRAD | Mean: -0.00430 | Std: 1.41296 | Min: -5.91408 | Max: 8.28982
[L1 B Grad] GRAD | Mean: -0.35271 | Std: 1.62956 | Min: -10.90779 | Max: 5.00861

Epoch 1000 | Loss: 0.625175
[Predictions] DATA | Mean: -0.04299 | Std: 0.83387 | Min: -5.47964 | Max: 10.51176
[L2 W Grad] GRAD | Mean: -0.14076 | Std: 1.76249 | Min: -24.34901 | Max: 21.83524
[L2 B Grad] GRAD | Mean: -0.28126 | Std: 1.51367 | Min: -6.72194 | Max: 6.65112
[L1 W Grad] GRAD | Mean: -0.01235 | Std: 1.95773 | Min: -12.02222 | Max: 14.08216
[L1 B Grad] GRAD | Mean: -0.55351 | Std: 2.15568 | Min: -10.05723 | Max: 6.66327

Epoch 1100 | Loss: 0.639393
[Predictions] DATA | Mean: 0.06377 | Std: 1.24697 | Min: -6.84618 | Max: 6.95330
[L2 W Grad] GRAD | Mean: -0.15904 | Std: 1.68776 | Min: -21.88231 | Max: 19.38834
[L2 B Grad] GRAD | Mean: -0.33829 | Std: 1.45638 | Min: -7.69086 | Max: 5.39031
[L1 W Grad] GRAD | Mean: -0.01902 | Std: 1.97470 | Min: -12.38131 | Max: 12.01138
[L1 B Grad] GRAD | Mean: -0.72802 | Std: 2.12330 | Min: -8.42581 | Max: 5.90343

Epoch 1200 | Loss: 1.036294
[Predictions] DATA | Mean: 0.14254 | Std: 1.58477 | Min: -11.71307 | Max: 8.86017
[L2 W Grad] GRAD | Mean: -0.06690 | Std: 1.07748 | Min: -12.93801 | Max: 11.06407
[L2 B Grad] GRAD | Mean: -0.21771 | Std: 0.97065 | Min: -5.86823 | Max: 2.87851
[L1 W Grad] GRAD | Mean: -0.00030 | Std: 1.40883 | Min: -6.77378 | Max: 8.11486
[L1 B Grad] GRAD | Mean: -0.63608 | Std: 1.64044 | Min: -7.24140 | Max: 4.06046

Epoch 1300 | Loss: 0.989302
[Predictions] DATA | Mean: 0.15148 | Std: 1.57167 | Min: -10.90798 | Max: 8.00796
[L2 W Grad] GRAD | Mean: 0.04464 | Std: 1.18957 | Min: -15.29551 | Max: 12.84268
[L2 B Grad] GRAD | Mean: -0.06214 | Std: 0.83379 | Min: -3.82060 | Max: 3.63556
[L1 W Grad] GRAD | Mean: 0.04361 | Std: 1.52150 | Min: -8.94443 | Max: 10.58327
[L1 B Grad] GRAD | Mean: -0.39640 | Std: 1.45187 | Min: -7.89183 | Max: 3.13187

Epoch 1400 | Loss: 0.656278
[Predictions] DATA | Mean: 0.04304 | Std: 1.26596 | Min: -3.80018 | Max: 6.10969
[L2 W Grad] GRAD | Mean: 0.06403 | Std: 1.66495 | Min: -17.49015 | Max: 22.80328
[L2 B Grad] GRAD | Mean: -0.00507 | Std: 1.03210 | Min: -3.27281 | Max: 4.22851
[L1 W Grad] GRAD | Mean: 0.04668 | Std: 2.02451 | Min: -17.37446 | Max: 13.08104
[L1 B Grad] GRAD | Mean: -0.26234 | Std: 1.60785 | Min: -9.74673 | Max: 3.88172

Epoch 1500 | Loss: 0.731579
[Predictions] DATA | Mean: -0.10903 | Std: 0.96130 | Min: -3.75208 | Max: 10.32574
[L2 W Grad] GRAD | Mean: 0.00900 | Std: 1.51667 | Min: -17.63318 | Max: 20.56737
[L2 B Grad] GRAD | Mean: -0.04635 | Std: 0.83770 | Min: -3.03117 | Max: 4.08376
[L1 W Grad] GRAD | Mean: 0.03114 | Std: 1.85183 | Min: -17.24470 | Max: 12.44530
[L1 B Grad] GRAD | Mean: -0.27047 | Std: 1.42633 | Min: -10.35745 | Max: 3.76729

Epoch 1600 | Loss: 0.947146
[Predictions] DATA | Mean: -0.12575 | Std: 0.80152 | Min: -8.32411 | Max: 8.17918
[L2 W Grad] GRAD | Mean: -0.03277 | Std: 1.08086 | Min: -11.04944 | Max: 13.42897
[L2 B Grad] GRAD | Mean: -0.10251 | Std: 0.94572 | Min: -4.60576 | Max: 3.11171
[L1 W Grad] GRAD | Mean: 0.03787 | Std: 1.47042 | Min: -8.83105 | Max: 10.82699
[L1 B Grad] GRAD | Mean: -0.28674 | Std: 1.48766 | Min: -9.33587 | Max: 3.70430

Epoch 1700 | Loss: 0.893042
[Predictions] DATA | Mean: 0.05988 | Std: 0.80452 | Min: -13.84712 | Max: 2.84139
[L2 W Grad] GRAD | Mean: -0.03654 | Std: 1.18721 | Min: -14.80672 | Max: 17.56715
[L2 B Grad] GRAD | Mean: -0.14594 | Std: 1.35898 | Min: -5.89783 | Max: 5.64633
[L1 W Grad] GRAD | Mean: 0.03166 | Std: 1.62121 | Min: -12.15450 | Max: 11.59428
[L1 B Grad] GRAD | Mean: -0.33226 | Std: 1.96981 | Min: -7.91595 | Max: 4.75943

Epoch 1800 | Loss: 0.645843
[Predictions] DATA | Mean: 0.24266 | Std: 0.95017 | Min: -11.41792 | Max: 4.00672
[L2 W Grad] GRAD | Mean: -0.03115 | Std: 1.50753 | Min: -17.22426 | Max: 19.08041
[L2 B Grad] GRAD | Mean: -0.17284 | Std: 1.46248 | Min: -6.40217 | Max: 8.16922
[L1 W Grad] GRAD | Mean: -0.02271 | Std: 1.92057 | Min: -15.43960 | Max: 13.65156
[L1 B Grad] GRAD | Mean: -0.44636 | Std: 2.08313 | Min: -9.13643 | Max: 5.28499

Epoch 1900 | Loss: 0.659647
[Predictions] DATA | Mean: 0.17815 | Std: 1.27554 | Min: -12.61378 | Max: 6.69264
[L2 W Grad] GRAD | Mean: -0.01684 | Std: 1.39585 | Min: -16.62835 | Max: 20.61012
[L2 B Grad] GRAD | Mean: -0.17601 | Std: 1.71142 | Min: -8.20888 | Max: 9.63599
[L1 W Grad] GRAD | Mean: -0.05304 | Std: 1.78995 | Min: -12.85162 | Max: 10.02651
[L1 B Grad] GRAD | Mean: -0.51723 | Std: 2.02081 | Min: -11.10658 | Max: 3.82737

Antrenament finalizat.

Verificare (Primele 5 exemple - valori reale):
Predicție       | Real (Target)   | Diferență      
--------------------------------------------------
0.6302          | 1.0300          | 0.3998         
3.2442          | 3.8210          | 0.5768         
2.2396          | 1.7260          | 0.5136         
-0.2235         | 0.9340          | 1.1575         
1.6329          | 0.9650          | 0.6679

Kernels.cuh:
#pragma once

// libs
#include <curand_kernel.h>
#include <cstdint>

// macro
#define TILE_WIDTH 32
using float32_t = float;

__global__ void matmul_kernel_tiled(
    const float32_t *A, 
    const float32_t *B, 
    float32_t *C, 
    const int32_t M, 
    const int32_t N, 
    const int32_t K
);

__global__ void matadd_kernel_tiled(
    const float32_t *A, 
    const float32_t *X, 
    float32_t *B, 
    const int32_t M, 
    const int32_t N
);

__global__ void ReLU_kernel_tiled(
    const float *In, 
    float *Out, 
    const u_int32_t M, 
    const uint32_t N
);

__global__ void populate_normal(
    float32_t *A, 
    uint32_t M, 
    uint32_t N, 
    float32_t std_dev,
    const uint64_t seed
);

__global__ void matmul_backward_X_kernel(
    const float32_t* grad_Y_out,
    const float32_t* W_in,
    float32_t* grad_X_in,
    const uint32_t M, 
    const uint32_t N, 
    const uint32_t K
);

__global__ void matmul_backward_W_kernel(
    const float32_t *X_in,
    const float32_t *grad_Y_out,
    float32_t *grad_W_in,
    const u_int32_t M, 
    const u_int32_t N, 
    const uint32_t K
);

__global__ void tensor_add_grad_kernel(
    const float32_t* src, 
    float32_t* dst, 
    int32_t size
);

__global__ void sum_rows_grad_kernel(
    const float32_t* src, 
    float32_t* dst, 
    int32_t M, 
    int32_t N
);

__global__ void relu_backward_kernel(
    const float32_t* grad_out, 
    const float32_t* input_data, 
    float32_t* grad_in, 
    int32_t size
);

__global__ void mse_forward_kernel(
    const float32_t *preds, 
    const float32_t *targets, 
    float32_t *loss_out, 
    const uint32_t N
);

__global__ void mse_div_kernel(
    float32_t *loss_out,
    const uint32_t N
);

__global__ void mse_backward_kernel(
    const float32_t *predictions,
    const float32_t *targets,
    const float32_t *grad_loss_scalar,
    float32_t *grad_predictions,
    const uint32_t N
);

__global__ void adam_step_kernel(
    float32_t* params,
    const float32_t* grads,
    float32_t* m,
    float32_t* v,
    const uint32_t size,
    const float32_t beta1,
    const float32_t beta2,
    const float32_t epsilon,
    const float32_t alpha,
    const float32_t beta1_corr,
    const float32_t beta2_corr
);


Launchers.cuh:
#pragma once

// libs
#include <cuda_runtime.h>
#include <algorithm>
#include <memory>
#include <chrono>

#include <core/Tensor.h>

// macro
#define TILE_WIDTH 32
using float32_t = float;

void launch_matmul_tiled(
    float *A, 
    float *B, 
    float *C, 
    int M, 
    int N, 
    int K, 
    cudaStream_t stream = 0
);

void launch_matadd_tiled(
    float *A, 
    float *X, 
    float *B, 
    int M, 
    int N, 
    cudaStream_t stream = 0
);

void launch_ReLU_tiled(
    float *In, 
    float *Out, 
    int M, 
    int N, 
    cudaStream_t stream = nullptr
);

void launch_zero_population(
    float *A, 
    int M, 
    int N, 
    cudaStream_t stream = nullptr
);

void launch_ones_population(
    float *A, 
    int M, 
    int N, 
    cudaStream_t stream = nullptr
);

void launch_normal_population(
    float *A, 
    int M, 
    int N, 
    float32_t std_dev,
    cudaStream_t stream = nullptr
);

void launch_matmul_grad_X(
    const float* grad_Y_out, 
    const float* W_in, 
    float* grad_X_in, 
    int M, 
    int N,
    int K, 
    cudaStream_t stream
);

void launch_matmul_grad_W(
    const float32_t *X_in, 
    const float32_t *grad_Y_out, 
    float32_t *grad_W_in, 
    int M, 
    int N, 
    int K, 
    cudaStream_t stream
);

void launch_tensor_add_grad(
    const float* src, 
    float* dst, 
    int size, 
    cudaStream_t stream
);

void launch_sum_rows_grad(
    const float* src, 
    float* dst, 
    int M, 
    int N, 
    cudaStream_t stream
);

void launch_relu_backward(
    const float* grad_out, 
    const float* input_data, 
    float* grad_in, 
    int size, 
    cudaStream_t stream
);

void launch_mse_backward(
    const float* preds, 
    const float* targets, 
    const float* grad_loss, 
    float* grad_preds, 
    int N, 
    cudaStream_t stream
);

void launch_mse_forward(
    const float* preds,
    const float* targets, 
    float* loss_out, 
    int N, 
    cudaStream_t stream
);

void launch_adam_step(
    float32_t* params, 
    const float32_t* grads, 
    float32_t* m, 
    float32_t* v, 
    int size, 
    float32_t beta1, 
    float32_t beta2, 
    float32_t epsilon, 
    float32_t lr, 
    const float32_t beta1_corr,
    const float32_t beta2_corr,
    cudaStream_t stream
);

adam_step_kernel.cu:
// headers
#include <backend/Kernels.cuh>

__global__ void adam_step_kernel(
    float32_t* params,
    const float32_t* grads,
    float32_t* m,
    float32_t* v,
    const uint32_t size,
    const float32_t beta1,
    const float32_t beta2,
    const float32_t epsilon,
    const float32_t alpha,
    const float32_t beta1_corr,
    const float32_t beta2_corr
) {
    const uint32_t idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx < size) {
        const float32_t g = grads[idx];
        
        // Update moment m
        m[idx] = beta1 * m[idx] + (1.0f - beta1) * g;
        
        // Update moment v
        v[idx] = beta2 * v[idx] + (1.0f - beta2) * g * g;

        // Bias correction
        const float32_t m_hat = m[idx] / beta1_corr;
        const float32_t v_hat = v[idx] / beta2_corr;

        // Update parameter
        params[idx] -= alpha * (m_hat / (sqrtf(v_hat) + epsilon));
    }
}

matadd_kernel.cu:
// headers
#include <backend/Kernels.cuh>

__global__ void matadd_kernel_tiled(const float *A, const float *X, float *B, const int M, const int N) {
    const uint32_t global_col = blockIdx.x * blockDim.x + threadIdx.x;
    const uint32_t global_row = blockIdx.y * blockDim.y + threadIdx.y;

    if (global_row < M && global_col < N) {
        const uint32_t index = global_row * N + global_col;
        B[index] = A[index] + X[global_col];
    }
} 


matmul_backward_W_kernel.cu:
// headers
#include <backend/Kernels.cuh>

// grad_W = X.T * gard_Y
#include <backend/Kernels.cuh>

__global__ void matmul_backward_W_kernel(
    const float32_t *X_in,          // M x N
    const float32_t *grad_Y_out,    // M x K
    float32_t *grad_W_in,           // N x K
    const uint32_t M, const uint32_t N, const uint32_t K)
{
    // N = Input Features
    // K = Output Features
    
    // Grid-ul este mapat pe dimensiunea Greutatilor (grad_W_in), adica N x K
    const uint32_t row = blockIdx.y * blockDim.y + threadIdx.y; // Range [0, N)
    const uint32_t col = blockIdx.x * blockDim.x + threadIdx.x; // Range [0, K)

    if (row < N && col < K) {
        float32_t val = 0.0f;
        for (uint32_t m = 0; m < M; ++m) {
            // Formula: grad_W = X^T * grad_Y
            // Elementul: X[m, row] * grad_Y[m, col]
            val += X_in[m * N + row] * grad_Y_out[m * K + col];
        }
        atomicAdd(&grad_W_in[row * K + col], val);
    }
}

matmult_backward_X_kernel.cu:
// headers
#include <backend/Kernels.cuh>

// gradientul fata de stratul anterior (inputul X al stratului curent)
// grad_x_in=grad_y_out*W.T
#include <backend/Kernels.cuh>

__global__ void matmul_backward_X_kernel(
    const float32_t *grad_Y_out, // M x K
    const float32_t *W_in,       // N x K (Aici e "invers" fata de forward)
    float32_t *grad_X_in,        // M x N
    const uint32_t M, const uint32_t N, const uint32_t K)
{
    // M = Batch Size
    // N = Input Features (ex: 32)
    // K = Output Features (ex: 1)
    
    // Grid-ul este mapat pe dimensiunea Output-ului (grad_X_in), adica M x N
    const uint32_t row = blockIdx.y * blockDim.y + threadIdx.y; // Range [0, M)
    const uint32_t col = blockIdx.x * blockDim.x + threadIdx.x; // Range [0, N)

    if (row < M && col < N) {
        float32_t val = 0.0f;
        for (uint32_t k = 0; k < K; ++k) {
            // Formula: grad_X = grad_Y * W^T
            // Elementul: grad_Y[row, k] * W[col, k] 
            // (Nota: W este stocat ca N x K, deci W[col, k] este corect pentru transpus)
            val += grad_Y_out[row * K + k] * W_in[col * K + k];
        }
        atomicAdd(&grad_X_in[row * N + col], val);
    }
}

matmul_kernel.cu:
// headers
#include <backend/Kernels.cuh>

__global__ void matmul_kernel_tiled(const float *A, const float *B, float *C, int M, int N, int K) {
    __shared__ float s_A[TILE_WIDTH][TILE_WIDTH];
    __shared__ float s_B[TILE_WIDTH][TILE_WIDTH];

    const uint32_t tx = threadIdx.x;
    const uint32_t ty = threadIdx.y;

    const uint32_t global_row = TILE_WIDTH * blockIdx.y + ty;
    const uint32_t global_col = TILE_WIDTH * blockIdx.x + tx;

    const uint32_t tile_num_reduction=(N+TILE_WIDTH-1)/TILE_WIDTH;
    float32_t val = 0.0f;

    for(int m = 0; m < tile_num_reduction; m += 1) {
        const uint32_t global_read_row_A = global_row;
        const uint32_t global_read_col_A = m * TILE_WIDTH + tx;

        const uint32_t global_read_row_B = m * TILE_WIDTH + ty;
        const uint32_t global_read_col_B = global_col;
        
        if(global_read_col_A < N && global_read_row_A < M) {
            s_A[ty][tx] = A[global_read_row_A * N + global_read_col_A];
        }
        else {
            s_A[ty][tx] = 0.0f;
        }

        if(global_read_col_B < K && global_read_row_B < N) {
            s_B[ty][tx] = B[global_read_row_B * K + global_read_col_B];
        }
        else {
            s_B[ty][tx] = 0.0f;
        }

        __syncthreads();

        for (int k = 0; k < TILE_WIDTH; ++k) {
            val += s_A[ty][k] * s_B[k][tx];
        }

        __syncthreads();
    } 

    if (global_row < M && global_col < K) {
        C[global_row * K + global_col] = val;
    }
}

mse_backward_kernel.cu:
// headers
#include <backend/Kernels.cuh>

__global__ void mse_backward_kernel(
    const float32_t *predictions,
    const float32_t *targets,
    const float32_t *grad_loss_scalar,
    float32_t *grad_predictions,
    const uint32_t N)
{
    const uint32_t idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx < N) {
        const float32_t diff = predictions[idx] - targets[idx];
        const float32_t factor = 2.0f / static_cast<float>(N);
        const float32_t local_grad = factor * diff * grad_loss_scalar[0];
        grad_predictions[idx] = local_grad;
    }
}

mse_div_kernel.cu:
// headers
#include <backend/Kernels.cuh>

__global__ void mse_div_kernel(float32_t *loss_out,const uint32_t N) {
    if (threadIdx.x == 0 && blockIdx.x == 0) {
        *loss_out /= static_cast<float32_t>(N);
    }
}


mse_forward_kernel.cu:
// headers
#include <backend/Kernels.cuh>

__global__ void mse_forward_kernel(const float32_t *preds, const float32_t *targets, float32_t *loss_out, const uint32_t N) {
    const uint32_t idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (idx < N) {
        const float32_t diff = preds[idx] - targets[idx];
        const float32_t sq_diff = diff * diff;
        atomicAdd(loss_out, sq_diff);
    }
}



populate_normal_kernel.cu:
// headers
#include <backend/Kernels.cuh>

__global__ void populate_normal(float32_t *A, uint32_t M, uint32_t N, float32_t std_dev, const uint64_t seed) {
    const uint32_t tid0 = blockIdx.x * blockDim.x + threadIdx.x;
    const uint32_t stride = gridDim.x * blockDim.x;
    const uint32_t total = M * N;

    curandStatePhilox4_32_10_t state;
    curand_init(seed, tid0, 0, &state);

    for (uint32_t i = tid0; i < total; i += stride) {
        // Înmulțim cu std_dev pentru a scala distribuția
        A[i] = curand_normal(&state) * std_dev; 
    }
}

ReLU_backward_kernel.cu:
// headers
#include <backend/Kernels.cuh>

__global__ void relu_backward_kernel(const float* grad_out, const float* input_data, float* grad_in, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        float mask = (input_data[idx] > 0.0f) ? 1.0f : 0.0f;
        grad_in[idx] += grad_out[idx] * mask;
    }
}


ReLU_kernel.cu:
// headers
#include <backend/Kernels.cuh>

__global__ void ReLU_kernel_tiled(const float *In, float *Out, const u_int32_t M, const uint32_t N) {
    const uint32_t global_col = blockIdx.x * blockDim.x + threadIdx.x;
    const uint32_t global_row = blockIdx.y * blockDim.y + threadIdx.y;

    if (global_row < M && global_col < N) {
        uint32_t index = global_row * N + global_col;
        Out[index] = fmaxf(0.0f, In[index]);
    }
}

sum_rows_grad_kernel.cu:
// headers
#include <backend/Kernels.cuh>

__global__ void sum_rows_grad_kernel(const float* src, float* dst, int M, int N) {
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    if (col < N) {
        float sum = 0.0f;
        for (int row = 0; row < M; ++row) {
            sum += src[row * N + col];
        }
        dst[col] += sum;
    }
}

tensor_add_grad_kernel.cu:
// headers
#include <backend/Kernels.cuh>

__global__ void tensor_add_grad_kernel(const float* src, float* dst, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        dst[idx] += src[idx];
    }
}

Launcher.cu:
// headers
#include <backend/Kernels.cuh>
#include <backend/Launchers.cuh>

using float32_t = float;

void launch_matmul_tiled(float32_t *A, float32_t *B, float32_t *C, int M, int N, int K, cudaStream_t stream) {
    dim3 block(TILE_WIDTH, TILE_WIDTH);
    dim3 grid((K + block.x - 1) / block.x, (M + block.y - 1) / block.y);

    matmul_kernel_tiled<<<grid, block, 0, stream>>>(A, B, C, M, N, K);
}

void launch_matadd_tiled(float32_t *A, float32_t *X, float32_t *B, int M, int N, cudaStream_t stream) {
    dim3 block(TILE_WIDTH, TILE_WIDTH);
    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);

    matadd_kernel_tiled<<<grid, block, 0, stream>>>(A, X, B, M, N);
}

void launch_zero_population(float32_t *A, int M, int N, cudaStream_t stream){
    size_t total = static_cast<size_t>(M) * static_cast<size_t>(N);
    cudaMemsetAsync(A, 0, total * sizeof(float32_t), stream);

    cudaStreamSynchronize(stream);
}

void launch_ones_population(float32_t *A, int M, int N, cudaStream_t stream){
    size_t total = static_cast<size_t>(M) * static_cast<size_t>(N);
    cudaMemsetAsync(A, 1, total * sizeof(float32_t), stream);

    cudaStreamSynchronize(stream);
}

void launch_normal_population(float32_t *A, int M, int N, float32_t std_dev, cudaStream_t stream) {
    size_t total = static_cast<size_t>(M) * static_cast<size_t>(N);

    int device;
    cudaGetDevice(&device);
    cudaDeviceProp device_props{};
    cudaGetDeviceProperties(&device_props, device);

    int threads = std::min(256, device_props.maxThreadsPerBlock);
    size_t blocks = (total + threads - 1) / threads;

    int activeBlocksPerSm = 0;
    cudaOccupancyMaxActiveBlocksPerMultiprocessor(
        &activeBlocksPerSm,
        populate_normal,    
        threads,
        0                    
    );

    size_t max_blocks = static_cast<size_t>(device_props.multiProcessorCount) * static_cast<size_t>(std::max(1, activeBlocksPerSm));
    if (blocks > max_blocks) blocks = max_blocks;

    unsigned long long seed = static_cast<unsigned long long>(
        std::chrono::high_resolution_clock::now().time_since_epoch().count()
    );

    // seed=42;
    populate_normal<<<blocks, threads, 0, stream>>>(A, M, N, std_dev, seed);
}

void launch_ReLU_tiled(float32_t *In, float32_t *Out, int M, int N, cudaStream_t stream) {
    dim3 block(TILE_WIDTH, TILE_WIDTH);
    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);

    ReLU_kernel_tiled<<<grid, block, 0, stream>>>(In, Out, M, N);
}


void launch_matmul_grad_X(const float32_t* grad_Y_out, const float32_t* W_in, float32_t* grad_X_in,
                          const int M, const int N, const int K, cudaStream_t stream) {

    dim3 block(TILE_WIDTH, TILE_WIDTH);
    dim3 grid((N + TILE_WIDTH - 1) / TILE_WIDTH, (M + TILE_WIDTH - 1) / TILE_WIDTH);

    matmul_backward_X_kernel<<<grid, block, 0, stream>>>(grad_Y_out, W_in, grad_X_in, M, N, K);
}

void launch_matmul_grad_W(const float32_t *X_in, const float32_t *grad_Y_out, float32_t *grad_W_in,
                          const int M, const int N, const int K, cudaStream_t stream) {

    dim3 block(TILE_WIDTH, TILE_WIDTH);
    dim3 grid((K + TILE_WIDTH - 1) / TILE_WIDTH, (N + TILE_WIDTH - 1) / TILE_WIDTH);

    matmul_backward_W_kernel<<<grid, block, 0, stream>>>(X_in, grad_Y_out, grad_W_in, M, N, K);
}

void launch_tensor_add_grad(const float32_t* src, float32_t* dst, int size, cudaStream_t stream) {
    int threads = 256;
    int blocks = (size + threads - 1) / threads;
    tensor_add_grad_kernel<<<blocks, threads, 0, stream>>>(src, dst, size);
}

void launch_sum_rows_grad(const float32_t* src, float32_t* dst, int M, int N, cudaStream_t stream) {
    int threads = 256;
    int blocks = (N + threads - 1) / threads;
    sum_rows_grad_kernel<<<blocks, threads, 0, stream>>>(src, dst, M, N);
}

void launch_relu_backward(const float32_t* grad_out, const float32_t* input_data, float32_t* grad_in, int size,
    cudaStream_t stream) {
    int threads = 256;
    int blocks = (size + threads - 1) / threads;
    relu_backward_kernel<<<blocks, threads, 0, stream>>>(grad_out, input_data, grad_in, size);
}


void launch_mse_backward(
    const float* preds,
    const float* targets,
    const float* grad_loss,
    float* grad_preds,
    int N,
    cudaStream_t stream)
{
    size_t total = static_cast<size_t>(N);

    int device;
    cudaGetDevice(&device);
    cudaDeviceProp device_props{};
    cudaGetDeviceProperties(&device_props, device);

    int threads = std::min(256, device_props.maxThreadsPerBlock);
    size_t blocks = (total + threads - 1) / threads;

    mse_backward_kernel<<<blocks, threads, 0, stream>>>(
        preds, targets, grad_loss, grad_preds, N
    );
}

void launch_mse_forward(const float* preds, const float* targets, float* loss_out, int N, cudaStream_t stream) {
    size_t total = static_cast<size_t>(N);

    int device;
    cudaGetDevice(&device);
    cudaDeviceProp device_props{};
    cudaGetDeviceProperties(&device_props, device);

    int threads = std::min(256, device_props.maxThreadsPerBlock);
    size_t blocks = (total + threads - 1) / threads;

    mse_forward_kernel<<<blocks, threads, 0, stream>>>(preds, targets, loss_out, N);

    mse_div_kernel<<<1, 1, 0, stream>>>(loss_out, N);
}

void launch_adam_step(
    float32_t* params,
    const float32_t* grads,
    float32_t* m,
    float32_t* v,
    int size,
    float32_t beta1,
    float32_t beta2,
    float32_t epsilon,
    float32_t lr,
    const float32_t beta1_corr,
    const float32_t beta2_corr,
    cudaStream_t stream
) {
    int threads = 256;
    int blocks = (size + threads - 1) / threads;

    adam_step_kernel<<<blocks, threads, 0, stream>>>(
        params, grads, m, v, size, beta1, beta2, epsilon, lr, beta1_corr, beta2_corr
    );
}

Tensor.h:
#pragma once

// libs
#include <memory>
#include <vector>
#include <cuda_runtime.h>

namespace core {
    using float32_t = float;

    // forward declaration
    struct Function;

    struct CudaDeallocator {
        void operator()(void *ptr) const {
            if (ptr) {
                cudaFree(ptr);
            }
        }
    };

    class Tensor : public std::enable_shared_from_this<Tensor> {
    private:
        std::unique_ptr<float32_t, CudaDeallocator> data_ptr;
        std::unique_ptr<float32_t, CudaDeallocator> gradient_ptr;

        std::vector<uint32_t> shape;
        size_t size;

        bool requires_gradient;
        bool is_leaf;

        std::shared_ptr<Function> grad_function;

    public:
        explicit Tensor(const std::vector<uint32_t>& shape, bool requires_gardient = false, bool is_leaf = true);
        ~Tensor() = default;

        void to_device(const std::vector<float32_t> &host_data) const;
        std::vector<float32_t> to_host() const;
        std::vector<float32_t> grad_to_host() const;

        std::vector<uint32_t> get_shape() const;
        float32_t *get_data_ptr() const;

        bool requires_grad() const { return requires_gradient; }
        float32_t *get_gradient_ptr() const;
        void set_grad_fn(std::shared_ptr<Function> fn);

        void backward(bool seed_gradient = true) const;

        friend struct Function;
    };
};



Tensor.cpp:
// headers
#include <core/Tensor.h>
#include <core/Context.h>
#include <core/Functional.h>
#include <core/Autograd.h>

// libs
#include <cuda_runtime.h>
#include <stdexcept>
#include <string>

namespace core {
    Tensor::Tensor(const std::vector<uint32_t>& shape, const bool requires_gradient, const bool is_leaf) :
    shape(shape), 
    requires_gradient(requires_gradient),
    is_leaf(is_leaf),
    grad_function(nullptr)
    {
        size = 1;
        for (const uint32_t s : shape) {
            size *= s;
        }

        float32_t* raw_ptr_data = nullptr;
        size_t bytes = size * sizeof(float32_t);

        cudaError_t err = cudaMalloc(reinterpret_cast<void **>(&raw_ptr_data), bytes);

        if (err != cudaSuccess) {
            const std::string error_msg = "CUDA Error: " + std::string(cudaGetErrorString(err));
            throw std::runtime_error(error_msg);
        }

        data_ptr.reset(raw_ptr_data);

        if (requires_gradient) {
            float32_t* raw_ptr_grad = nullptr;
            bytes = size * sizeof(float32_t);
    
            err = cudaMalloc(reinterpret_cast<void **>(&raw_ptr_grad), bytes);
    
            if (err != cudaSuccess) {
                const std::string error_msg = "CUDA Error: " + std::string(cudaGetErrorString(err));
                throw std::runtime_error(error_msg);
            }
            const cudaStream_t& stream = CudaContext::getStream();

            pop_grad_zeros(this,stream);
    
            gradient_ptr.reset(raw_ptr_grad);
        }
    }

    void Tensor::to_device(const std::vector<float32_t>& host_data) const {
        if (host_data.size() != size) {
            throw std::length_error("Size mismatch: CPU array has different size to GPU tensor.");
        }

        if (const cudaError_t err = cudaMemcpyAsync(data_ptr.get(), host_data.data(), size * sizeof(float32_t), cudaMemcpyHostToDevice, CudaContext::getStream()); err != cudaSuccess) {
            throw std::runtime_error("To GPU error (Memcpy H2D)");
        }
    }

    std::vector<float32_t> Tensor::to_host() const {
        std::vector<float32_t> host_result(size);

        if (const cudaError_t err = cudaMemcpyAsync(host_result.data(), data_ptr.get(), size * sizeof(float32_t), cudaMemcpyDeviceToHost, CudaContext::getStream()); err != cudaSuccess) {
            throw std::runtime_error("To CPU error (Memcpy D2H)");
        }

        cudaStreamSynchronize(CudaContext::getStream());
        
        return host_result;
    }

    std::vector<float32_t> Tensor::grad_to_host() const {
        std::vector<float32_t> host_result(size);

        if (const cudaError_t err = cudaMemcpyAsync(host_result.data(), gradient_ptr.get(), size * sizeof(float32_t), cudaMemcpyDeviceToHost, CudaContext::getStream()); err != cudaSuccess) {
            throw std::runtime_error("To CPU error (Memcpy D2H)");
        }

        cudaStreamSynchronize(CudaContext::getStream());

        return host_result;
    }

    std::vector<uint32_t> Tensor::get_shape() const {
        return shape;
    };

    float32_t *Tensor::get_data_ptr() const {
        return data_ptr.get();
    };

    float32_t *Tensor::get_gradient_ptr() const {
        if (!gradient_ptr)
            return nullptr;
        return gradient_ptr.get();
    }

    void Tensor::set_grad_fn(std::shared_ptr<Function> fn) {
        grad_function=fn;
        is_leaf=false;
    }

    void Tensor::backward(bool seed_gradient) const {
        if (!requires_gradient) {
            throw std::runtime_error("Called backward() on a tensor that does not require gradients.");
        }

        if (seed_gradient) {
            size_t total_elements = 1;
            for (auto s : shape) total_elements *= s;

            std::vector<float> host_ones(total_elements, 1.0f);

            cudaMemcpyAsync(
                gradient_ptr.get(),
                host_ones.data(),
                total_elements * sizeof(float),
                cudaMemcpyHostToDevice,
                CudaContext::getStream()
            );
        }

        if (grad_function)
            grad_function->apply_backward();
    }
};

Functional.h:
#pragma once

// libs
#include <memory>

namespace core {
    void matmul(const std::shared_ptr<Tensor>& A, const std::shared_ptr<Tensor>& B,
        std::shared_ptr<Tensor>& C, const cudaStream_t& stream );
    void matadd(const std::shared_ptr<Tensor>& A, const std::shared_ptr<Tensor>& X,
        std::shared_ptr<Tensor>& B,const cudaStream_t& stream);
    std::shared_ptr<Tensor> relu(const std::shared_ptr<Tensor> &In,
        const cudaStream_t& stream );
    void pop_data_zeros(const std::shared_ptr<Tensor> &A,
        const cudaStream_t& stream );
    void pop_grad_zeros(const std::shared_ptr<Tensor> &A,
        const cudaStream_t& stream );
    void pop_grad_zeros(Tensor *A,
        const cudaStream_t& stream );
    void pop_grad_ones(const std::shared_ptr<Tensor> &A,
        const cudaStream_t& stream );
    void pop_grad_ones(Tensor *A,
        const cudaStream_t& stream );
    void pop_data_normal(const std::shared_ptr<Tensor> &A, float std_dev, 
        const cudaStream_t& stream );
    std::shared_ptr<Tensor> mse_loss(const std::shared_ptr<Tensor>& preds, const std::shared_ptr<Tensor>& targets,
        const cudaStream_t& stream );
};


Functional.cpp:
// headers
#include <core/Tensor.h>
#include <backend/Launchers.cuh>
#include <core/Context.h>
#include <core/Autograd.h>

// libs
#include <memory>

namespace core {
    void matmul(const std::shared_ptr<Tensor>& A, const std::shared_ptr<Tensor>& B,
        std::shared_ptr<Tensor>& C, const cudaStream_t& stream = CudaContext::getStream()) {
        const uint32_t M = A->get_shape()[0];
        const uint32_t N = A->get_shape()[1];
        const uint32_t K = B->get_shape()[1];

        bool needs_grad = A->requires_grad() || B->requires_grad();

        C = std::make_shared<Tensor>(std::vector<uint32_t>{M, K}, needs_grad, false);

        launch_matmul_tiled(
            A->get_data_ptr(), 
            B->get_data_ptr(), 
            C->get_data_ptr(), 
            M, N, K,
            stream
        );

        if (needs_grad) {
            auto node = std::make_shared<MatMulFunction>(A, B, C);
            C->set_grad_fn(node);
        }

    }
    
    void matadd(const std::shared_ptr<Tensor>& A, const std::shared_ptr<Tensor>& X,
        std::shared_ptr<Tensor>& B,const cudaStream_t& stream = CudaContext::getStream()) {
        uint32_t M = A->get_shape()[0];
        uint32_t N = A->get_shape()[1];

        bool needs_grad = A->requires_grad() || X->requires_grad();

        B = std::make_shared<Tensor>(std::vector<uint32_t>{M, N}, needs_grad, false);

        launch_matadd_tiled(
            A->get_data_ptr(), 
            X->get_data_ptr(), 
            B->get_data_ptr(), 
            M, N,
            stream
        );

        if (needs_grad) {
            auto node = std::make_shared<AddFunction>(A, X, B);
            B->set_grad_fn(node);
        }
    }

    std::shared_ptr<Tensor> relu(const std::shared_ptr<Tensor>& In,
        const cudaStream_t& stream = CudaContext::getStream()) {
        uint32_t M = In->get_shape()[0];
        uint32_t N = In->get_shape()[1];

        bool needs_grad = In->requires_grad();

        auto Out = std::make_shared<Tensor>(std::vector<uint32_t>{M, N},needs_grad,false);

        launch_ReLU_tiled(
            In->get_data_ptr(), 
            Out->get_data_ptr(),
            M, N,
            stream
        );

        if (needs_grad) {
            auto node = std::make_shared<ReLUFunction>(In, Out);
            Out->set_grad_fn(node);
        }

        return Out;
    }

    void pop_data_zeros(const std::shared_ptr<Tensor>& A,
        const cudaStream_t& stream = CudaContext::getStream()){
        int M = A->get_shape()[0];
        int N = A->get_shape()[1];

        if (N == 0)
            N = 1;


        launch_zero_population(A->get_data_ptr(), M, N, stream);
    }

    void pop_grad_zeros(const std::shared_ptr<Tensor>& A, const cudaStream_t& stream = CudaContext::getStream()){
        int M = A->get_shape()[0];
        int N = A->get_shape()[1];

        if (N == 0)
            N = 1;


        launch_zero_population(A->get_gradient_ptr(), M, N, stream);
    }

    void pop_grad_zeros(Tensor *A, const cudaStream_t& stream = CudaContext::getStream()){
        int M = A->get_shape()[0];
        int N = A->get_shape()[1];

        if (N == 0)
            N = 1;


        launch_zero_population(A->get_gradient_ptr(), M, N, stream);
    }

    void pop_grad_ones(const std::shared_ptr<Tensor>& A,
        const cudaStream_t& stream = CudaContext::getStream()){
        int M = A->get_shape()[0];
        int N = A->get_shape()[1];

        if (N == 0)
            N = 1;


        launch_ones_population(A->get_gradient_ptr(), M, N, stream);
    }

    void pop_grad_ones(Tensor *A,
        const cudaStream_t& stream = CudaContext::getStream()){
        int M = A->get_shape()[0];
        int N = A->get_shape()[1];

        if (N == 0)
            N = 1;

        launch_ones_population(A->get_gradient_ptr(), M, N, stream);
    }

    void pop_data_normal(const std::shared_ptr<Tensor>& A, float std_dev,
        const cudaStream_t& stream = CudaContext::getStream()){
        int M = A->get_shape()[0];
        int N = A->get_shape()[1];

        if (N == 0)
            N = 1;


        launch_normal_population(A->get_data_ptr(), M, N, std_dev, stream);
    }


    std::shared_ptr<Tensor> mse_loss(const std::shared_ptr<Tensor>& preds, const std::shared_ptr<Tensor>& targets,
        const cudaStream_t& stream = CudaContext::getStream()) {
        uint32_t N = 1;
        for (auto s : preds->get_shape()) N *= s;

        bool needs_grad = preds->requires_grad();

        auto loss = std::make_shared<Tensor>(std::vector<uint32_t>{1}, needs_grad, false);

        cudaMemsetAsync(loss->get_data_ptr(), 0, sizeof(float32_t), stream);

        launch_mse_forward(
            preds->get_data_ptr(), 
            targets->get_data_ptr(), 
            loss->get_data_ptr(), 
            N, 
            stream);

        if (needs_grad) {
            auto node = std::make_shared<MSEFunction>(preds, targets, loss);
            loss->set_grad_fn(node);
        }

        return loss;
    }
};


Autograd.h:
#pragma once

#include <core/Tensor.h>

namespace core {
    struct Function {
        virtual void apply_backward() = 0;
        virtual ~Function() = default;
    };

    struct MatMulFunction : public Function {
        std::shared_ptr<Tensor> X_input;
        std::shared_ptr<Tensor> W_input;
        std::weak_ptr<Tensor> Y_output;

        MatMulFunction(std::shared_ptr<Tensor> x, std::shared_ptr<Tensor> w, std::shared_ptr<Tensor> y);

        void apply_backward() override;
    };

    struct AddFunction : public Function {
        std::shared_ptr<Tensor> X_input;
        std::shared_ptr<Tensor> bias_input;
        std::weak_ptr<Tensor> Y_output;

        AddFunction(std::shared_ptr<Tensor> x, std::shared_ptr<Tensor> bias, std::shared_ptr<Tensor> y);

        void apply_backward() override;
    };

    struct ReLUFunction : public Function {
        std::shared_ptr<Tensor> Input;
        std::weak_ptr<Tensor> Output;

        ReLUFunction(std::shared_ptr<Tensor> in, std::shared_ptr<Tensor> out);

        void apply_backward() override ;
    };

    struct MSEFunction : public Function {
        std::shared_ptr<Tensor> predictions;
        std::shared_ptr<Tensor> targets;
        std::weak_ptr<Tensor> output_loss;

        MSEFunction(const std::shared_ptr<Tensor>& preds,
                    const std::shared_ptr<Tensor>& targs,
                    const std::shared_ptr<Tensor>& out);

        void apply_backward() override;
    };

    struct FlashAttentionFunction : public Function {
        std::shared_ptr<Tensor> Q_input;
        std::shared_ptr<Tensor> K_input;
        std::shared_ptr<Tensor> V_input;
        std::shared_ptr<Tensor> L_vec; // LogSumExp statistics din Forward
        std::weak_ptr<Tensor> Output;  // Weak ptr pentru a evita ciclurile

        FlashAttentionFunction(
            std::shared_ptr<Tensor> q,
            std::shared_ptr<Tensor> k,
            std::shared_ptr<Tensor> v,
            std::shared_ptr<Tensor> out,
            std::shared_ptr<Tensor> l_vec
        );

        void apply_backward() override;
    };
};

Autograd.cpp:
#include <core/Tensor.h>
#include <backend/Launchers.cuh>
#include <core/Context.h>
#include <core/Autograd.h>


namespace core {
    MatMulFunction::MatMulFunction(std::shared_ptr<Tensor> x, std::shared_ptr<Tensor> w, std::shared_ptr<Tensor> y)
        : X_input(x), W_input(w), Y_output(y) {}

    void MatMulFunction::apply_backward()  {
        const uint32_t M = X_input->get_shape()[0];
        const uint32_t N = X_input->get_shape()[1];
        const uint32_t K = W_input->get_shape()[1];

        const auto grad_out_ptr = Y_output.lock()->get_gradient_ptr();

        if (X_input->requires_grad()) {
            launch_matmul_grad_X(grad_out_ptr,W_input->get_data_ptr(), X_input->get_gradient_ptr(),M, N, K,
                CudaContext::getStream());
            X_input->backward(false);
        }

        if (W_input->requires_grad()) {
            launch_matmul_grad_W(X_input->get_data_ptr(),grad_out_ptr,W_input->get_gradient_ptr(), M, N, K,
                CudaContext::getStream());
            W_input->backward(false);
        }
    }


    AddFunction::AddFunction(std::shared_ptr<Tensor> x, std::shared_ptr<Tensor> bias, std::shared_ptr<Tensor> y)
            : X_input(x), bias_input(bias), Y_output(y) {}

    void AddFunction::apply_backward() {
        auto out_ptr = Y_output.lock();
        if (!out_ptr) return;

        const auto grad_out_ptr = out_ptr->get_gradient_ptr();
        uint32_t M = out_ptr->get_shape()[0];
        uint32_t N = out_ptr->get_shape()[1];
        uint32_t size = M * N;

        if (X_input->requires_grad()) {
            launch_tensor_add_grad(grad_out_ptr,X_input->get_gradient_ptr(),size, CudaContext::getStream());
            X_input->backward(false);
        }

        if (bias_input->requires_grad()) {
            bool is_bias = (bias_input->get_shape()[0] == 1 && M > 1);

            if (is_bias)
                launch_sum_rows_grad(grad_out_ptr,bias_input->get_gradient_ptr(), M, N, CudaContext::getStream());
            else
                launch_tensor_add_grad(grad_out_ptr,bias_input->get_gradient_ptr(),size,CudaContext::getStream());

            bias_input->backward(false);
        }
    }


    ReLUFunction::ReLUFunction(std::shared_ptr<Tensor> in, std::shared_ptr<Tensor> out)
    : Input(in), Output(out) {}

    void ReLUFunction::apply_backward()  {
        auto out_ptr = Output.lock();
        if (!out_ptr) return;

        uint32_t M = Input->get_shape()[0];
        uint32_t N = Input->get_shape()[1];
        uint32_t size = M * N;

        if (Input->requires_grad()) {
            launch_relu_backward(out_ptr->get_gradient_ptr(), Input->get_data_ptr(), Input->get_gradient_ptr(),
                size, CudaContext::getStream());
            Input->backward(false);
        }
    }


    MSEFunction::MSEFunction(const std::shared_ptr<Tensor>& preds,
                    const std::shared_ptr<Tensor>& targs,
                    const std::shared_ptr<Tensor>& out)
            : predictions(preds), targets(targs), output_loss(out) {}
    
    void MSEFunction::apply_backward()  {
        auto loss_ptr = output_loss.lock();
        if (!loss_ptr) return;

        uint32_t N = 1;
        for(auto s : predictions->get_shape()) N *= s;

        if (predictions->requires_grad()) {
            launch_mse_backward(
                predictions->get_data_ptr(),
                targets->get_data_ptr(),
                loss_ptr->get_gradient_ptr(),
                predictions->get_gradient_ptr(),
                N,
                CudaContext::getStream()
            );

            predictions->backward(false);
        }
    }
}


Adam.h:
#pragma once

// libs
#include <vector>
#include <memory>
#include <core/Tensor.h>

namespace optim {
    class Adam {
    private:
        std::vector<std::shared_ptr<core::Tensor>> parameters;

        std::vector<std::shared_ptr<core::Tensor>> m_states;
        std::vector<std::shared_ptr<core::Tensor>> v_states;

        float lr;
        float beta1;
        float beta2;
        float epsilon;
        int step_count;

    public:
        Adam(
            std::vector<std::shared_ptr<core::Tensor>> params, 
            float lr = 0.001f, 
            float beta1 = 0.9f, 
            float beta2 = 0.999f, 
            float eps = 1e-8f
        );

        void step();
        void zero_grad();
    };
}

Adam.cpp:
// headers
#include <core/Adam.h>
#include <backend/Launchers.cuh>
#include <core/Context.h>
#include <core/Functional.h>

// libs
#include <cmath>

namespace optim {
    Adam::Adam(std::vector<std::shared_ptr<core::Tensor>> params, float lr, float beta1, float beta2, float eps)
        : parameters(params), lr(lr), beta1(beta1), beta2(beta2), epsilon(eps), step_count(0) 
    {
        for (const auto& p : parameters) {
            auto m = std::make_shared<core::Tensor>(p->get_shape(), false, false);
            auto v = std::make_shared<core::Tensor>(p->get_shape(), false, false);

            const cudaStream_t& stream = CudaContext::getStream();

            core::pop_data_zeros(m,stream);
            core::pop_data_zeros(v,stream);

            m_states.push_back(m);
            v_states.push_back(v);
        }
    }

    void Adam::step() {
        step_count++;

        float bias_correction1 = 1.0f - std::pow(beta1, step_count);
        float bias_correction2 = 1.0f - std::pow(beta2, step_count);

        for (size_t i = 0; i < parameters.size(); ++i) {
            auto& param = parameters[i];
            
            if (!param->requires_grad() || param->get_gradient_ptr() == nullptr) {
                continue;
            }

            uint32_t size = 1;
            for(auto s : param->get_shape()) size *= s;

            launch_adam_step(
                param->get_data_ptr(),
                param->get_gradient_ptr(),
                m_states[i]->get_data_ptr(),
                v_states[i]->get_data_ptr(),
                size,
                beta1,
                beta2,
                epsilon,
                lr,
                bias_correction1,
                bias_correction2,
                CudaContext::getStream()
            );
        }
    }

    void Adam::zero_grad() {
        for (auto& param : parameters) {
            if (param->requires_grad()) {
                const cudaStream_t& stream = CudaContext::getStream();
                core::pop_grad_zeros(param,stream);
            }
        }
    }
}

Context.h:
#pragma once

// libs
#include <cuda_runtime.h>

struct CudaContext {
    inline static cudaStream_t current_stream = nullptr;
    
    static cudaStream_t getStream() {
        return current_stream;
    }
    
    static void setStream(cudaStream_t s) {
        current_stream = s;
    }
};


Module.h:
#pragma once

// headers
#include <core/Tensor.h>

// libs
#include <memory>
#include <vector>

namespace layers {
    class Module {
    public:
        virtual ~Module() = default;

        virtual std::shared_ptr<core::Tensor> forward(const std::shared_ptr<core::Tensor> &input) = 0;
        virtual std::vector<std::shared_ptr<core::Tensor>> parameters() {
            return {}; 
        }
    };
};

Linear.h:
#pragma once

// headers
#include <layers/Module.h>
#include <core/Tensor.h>

// libs
#include <memory>
#include <vector>
#include <cmath>

namespace layers {
    class Linear : public Module {
    public:
        std::shared_ptr<core::Tensor> weight;
        std::shared_ptr<core::Tensor> bias;

    public:
        Linear(uint32_t in_channels, uint32_t out_channels);

        std::shared_ptr<core::Tensor> forward(const std::shared_ptr<core::Tensor> &input) override;
        std::vector<std::shared_ptr<core::Tensor>> parameters() override;
    };
};


Linear.cpp:
// headers
#include <layers/Linear.h>
#include <core/Tensor.h>
#include <core/Functional.h>
#include <core/Context.h>

namespace layers {
    Linear::Linear(const uint32_t in_channels, const uint32_t out_channels) {
        // x: batch x in_channels
        // W: in_channels x out_channels
        // b: 1 x out_channels
        // y = x @ W + b -> y: batch x out_channels    
        weight = std::make_shared<core::Tensor>(std::vector<uint32_t>{in_channels,out_channels},true);
        bias = std::make_shared<core::Tensor>(std::vector<uint32_t>{1,out_channels},true);

        const cudaStream_t& stream = CudaContext::getStream();
        
        // Xavier Initialization
        // float std_dev = 1.0f / std::sqrt(static_cast<float>(in_channels));
        float std_dev = std::sqrt(2.0f / in_channels);
        
        pop_data_normal(weight, std_dev, stream);
        pop_data_zeros(bias,stream);
    }

    std::shared_ptr<core::Tensor> Linear::forward(const std::shared_ptr<core::Tensor> &input) {
        std::shared_ptr<core::Tensor> XW, Y;
        const cudaStream_t& stream = CudaContext::getStream();

        matmul(input,weight,XW,stream);
        matadd(XW, bias, Y,stream);
        return Y;
    }

    std::vector<std::shared_ptr<core::Tensor>> Linear::parameters() {
        return {weight, bias};
    }
}

ReLU.h:
#pragma once

// headers
#include <layers/Module.h>
#include <core/Functional.h>
#include <core/Tensor.h>

namespace layers {
    class ReLU : public Module {
    public:
        ReLU() = default;

        std::shared_ptr<core::Tensor> forward(const std::shared_ptr<core::Tensor> &In) override;
    };
};


ReLU.cpp:
// headers
#include <layers/ReLU.h>
#include <core/Context.h>

namespace layers {
    std::shared_ptr<core::Tensor> ReLU::forward(const std::shared_ptr<core::Tensor> &In) {
        const cudaStream_t& stream = CudaContext::getStream();
        return relu(In,stream);
    };
};


Loss.h:
#pragma once

// headers
#include <core/Tensor.h>

// libs
#include <memory>

namespace loss {
    class Loss {
    public:
        virtual ~Loss() = default;

        virtual std::shared_ptr<core::Tensor> forward(
            const std::shared_ptr<core::Tensor> &prediction, 
            const std::shared_ptr<core::Tensor> &target
        ) = 0;
    };
}

MSE.h:
#pragma once

// headers
#include <loss/Loss.h>

// libs
#include <stdexcept>

namespace loss {
    class MSE : public Loss {
    public:
        MSE() = default;

        std::shared_ptr<core::Tensor> forward(const std::shared_ptr<core::Tensor> &prediction, const std::shared_ptr<core::Tensor> &target) ;
    };
}

MSE.cpp:
// headers
#include <loss/MSE.h>
#include <core/Tensor.h>
#include <core/Functional.h>
#include <core/Context.h>

namespace loss {
    std::shared_ptr<core::Tensor> MSE::forward(const std::shared_ptr<core::Tensor> &prediction, const std::shared_ptr<core::Tensor> &target) {
        const cudaStream_t& stream = CudaContext::getStream();
        return core::mse_loss(prediction, target, stream);
    }
}

bindings.cpp:
#include <pybind11/pybind11.h>
#include <pybind11/stl.h>

#include <core/Tensor.h>
#include <layers/Module.h>
#include <layers/Linear.h>
#include <layers/ReLU.h>
#include <loss/MSE.h>
#include <core/Functional.h>
#include <core/Adam.h>

namespace py = pybind11;
using namespace core;
using namespace layers;

class PyModule : public Module {
public:
    using Module::Module;

    std::shared_ptr<Tensor> forward(const std::shared_ptr<Tensor> &input) override {
        PYBIND11_OVERRIDE_PURE(
            std::shared_ptr<Tensor>,
            Module,
            forward,
            input
        );
    }

    std::vector<std::shared_ptr<Tensor>> parameters() override {
        PYBIND11_OVERRIDE_PURE(
            std::vector<std::shared_ptr<Tensor>>,
            Module,
            parameters,
        );
    }
};

PYBIND11_MODULE(PureAttention, m) {
    m.doc() = "Deep Learning Framework";


    py::class_<Tensor, std::shared_ptr<Tensor>>(m, "Tensor")
        .def(py::init<std::vector<uint32_t>, bool>(), py::arg("shape"), py::arg("requires_grad")=false)
        .def("backward", [](Tensor& self) { self.backward(true); })
        .def("to_host", &Tensor::to_host)
        .def("to_device", &Tensor::to_device)
        .def("grad_to_host", &Tensor::grad_to_host)
        .def("shape", &Tensor::get_shape);

    py::class_<Module, PyModule, std::shared_ptr<Module>>(m, "Module")
        .def(py::init<>())
        .def("forward", &Module::forward)
        .def("parameters", &Module::parameters);

    py::class_<Linear, Module, std::shared_ptr<Linear>>(m, "Linear")
        .def(py::init<uint32_t, uint32_t>())
        .def("forward", &Linear::forward)
        .def("parameters", &Linear::parameters)
        .def_readwrite("weight", &Linear::weight)
        .def_readwrite("bias", &Linear::bias);

    py::class_<ReLU, Module, std::shared_ptr<ReLU>>(m, "ReLU")
        .def(py::init<>())
        .def("forward", &ReLU::forward)
        .def("parameters", &ReLU::parameters);

    py::class_<loss::MSE, std::shared_ptr<loss::MSE>>(m, "MSE")
        .def(py::init<>())
        .def("forward", py::overload_cast<const std::shared_ptr<Tensor>&, const std::shared_ptr<Tensor>&>(&loss::MSE::forward))
        .def("__call__", py::overload_cast<const std::shared_ptr<Tensor>&, const std::shared_ptr<Tensor>&>(&loss::MSE::forward));

    py::class_<optim::Adam, std::shared_ptr<optim::Adam>>(m, "Adam")
        .def(py::init<std::vector<std::shared_ptr<Tensor>>, float, float, float, float>(), 
             py::arg("params"), py::arg("lr")=0.001f, py::arg("beta1")=0.9f, py::arg("beta2")=0.999f, py::arg("eps")=1e-8f)
        .def("step", &optim::Adam::step)
        .def("zero_grad", &optim::Adam::zero_grad);
}

main.py:
import PureAttention as pa
import numpy as np
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import sys

# --- Funcție Ajutătoare pentru Debug ---
def debug_tensor(name, tensor_data, is_grad=False):
    """
    Calculează statistici pentru un tensor (listă sau numpy array).
    """
    arr = np.array(tensor_data)
    
    if arr.size == 0:
        return f"[{name}] EMPTY"

    msg = (f"[{name}] {'GRAD' if is_grad else 'DATA'} | "
           f"Mean: {arr.mean():.5f} | Std: {arr.std():.5f} | "
           f"Min: {arr.min():.5f} | Max: {arr.max():.5f}")
    
    # Verificare pentru NaN sau Infinit
    if np.isnan(arr).any():
        msg += " !!! CONTAINS NaN !!!"
    if np.isinf(arr).any():
        msg += " !!! CONTAINS Inf !!!"
        
    # Verificare pentru 'Dead Neurons' sau gradienți zero
    if is_grad and np.allclose(arr, 0):
        msg += " !!! ZERO GRADIENT (BLOCKAGE) !!!"
        
    return msg

class HousingModel(pa.Module):
    def __init__(self, input_dim):
        super().__init__()
        # Arhitectura: Input(8) -> Linear(32) -> ReLU -> Linear(1)
        self.l1 = pa.Linear(input_dim, 128)
        self.relu = pa.ReLU()
        self.l2 = pa.Linear(128, 128)
        self.relu = pa.ReLU()
        self.l3 = pa.Linear(128, 1)

    def forward(self, x):
        # Putem inspecta și output-urile intermediare dacă e nevoie, 
        # dar momentan ne bazăm pe gradienți.
        x = self.l1.forward(x)
        x = self.relu.forward(x)
        x = self.l2.forward(x)
        x = self.relu.forward(x)
        x = self.l3.forward(x)

        return x

    def parameters(self):
        return self.l1.parameters() + self.l2.parameters()

def main():
    # 1. Pregătirea datelor
    print("Se încarcă datele California Housing...")
    data = fetch_california_housing()
    X, y = data.data, data.target

    # Scalare
    scaler_X = StandardScaler()
    X = scaler_X.fit_transform(X)
    
    y = y.reshape(-1, 1)
    scaler_y = StandardScaler()
    y = scaler_y.fit_transform(y)

    X = X.astype(np.float32)
    y = y.astype(np.float32)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    BATCH_SIZE = 5000 # Batch mai mare pentru stabilitate
    X_batch = X_train[:BATCH_SIZE]
    y_batch = y_train[:BATCH_SIZE]
    
    input_data_flat = X_batch.flatten()
    target_data_flat = y_batch.flatten()

    B = X_batch.shape[0]
    IN = X_batch.shape[1]
    OUT = 1

    # 2. Setup Tensors
    input_tensor = pa.Tensor([B, IN], False)
    input_tensor.to_device(input_data_flat)

    target_tensor = pa.Tensor([B, OUT], False)
    target_tensor.to_device(target_data_flat)

    # 3. Model și Optimizer
    model = HousingModel(input_dim=IN)
    
    # Încearcă un LR mai mic dacă explodează, sau mai mare dacă stagnează
    LR = 0.00001 
    optimizer = pa.Adam(model.parameters(), lr=LR) 
    criterion = pa.MSE()

    print(f"Start training on {B} samples with LR={LR}...")

    # Verificăm inițializarea greutăților înainte de start
    print("\n--- Initial Weights Stats ---")
    print(debug_tensor("L1 Weights", model.l1.weight.to_host()))
    print(debug_tensor("L1 Bias", model.l1.bias.to_host()))
    print(debug_tensor("L2 Weights", model.l2.weight.to_host()))
    print("-" * 50)

    # 4. Training Loop
    epochs = 1000
    for i in range(epochs):
        optimizer.zero_grad()

        # Forward
        pred = model.forward(input_tensor)
        
        # Loss
        loss = criterion.forward(pred, target_tensor)

        # Backward
        loss.backward()
        
        # --- DEBUGGING PHASE (Înainte de step) ---
        # Verificăm starea la fiecare 50 epoci SAU dacă e prima epocă (să vedem dacă backprop merge din start)
        if i == 0 or i % 100 == 0:
            loss_val = loss.to_host()[0]
            print(f"\nEpoch {i} | Loss: {loss_val:.6f}")
            
            # 1. Verificăm output-ul rețelei (Predicțiile)
            # Dacă Std e 0, rețeaua prezice o constantă (colaps)
            print(debug_tensor("Predictions", pred.to_host()))

            # 2. Verificăm Gradienții pentru L2 (Ultimul strat)
            # Dacă aceștia sunt 0, eroarea nu ajunge la rețea (problema la MSE sau L2 backward)
            print(debug_tensor("L2 W Grad", model.l2.weight.grad_to_host(), is_grad=True))
            print(debug_tensor("L2 B Grad", model.l2.bias.grad_to_host(), is_grad=True))

            # 3. Verificăm Gradienții pentru L1 (Primul strat)
            # Dacă L2 are gradienți, dar L1 nu, înseamnă că ReLU a omorât totul sau Matmul Backward e greșit
            print(debug_tensor("L1 W Grad", model.l1.weight.grad_to_host(), is_grad=True))
            print(debug_tensor("L1 B Grad", model.l1.bias.grad_to_host(), is_grad=True))

            # Verificăm dacă greutățile au devenit NaN
            w1 = np.array(model.l1.weight.to_host())
            if np.isnan(w1).any():
                print("CRITICAL: Weights became NaN. Stopping.")
                break

        # Weight update
        optimizer.step()

    print("\nAntrenament finalizat.")
    
    # 5. Verificare Finală
    preds = np.array(pred.to_host())
    preds_reshaped = preds.reshape(-1, 1)
    preds_real = scaler_y.inverse_transform(preds_reshaped).flatten()
    targets_real = scaler_y.inverse_transform(y_batch).flatten()

    print("\nVerificare (Primele 5 exemple - valori reale):")
    print(f"{'Predicție':<15} | {'Real (Target)':<15} | {'Diferență':<15}")
    print("-" * 50)
    for k in range(5):
        p = preds_real[k]
        t = targets_real[k]
        print(f"{p:<15.4f} | {t:<15.4f} | {abs(p-t):<15.4f}")

if __name__ == "__main__":
    main()